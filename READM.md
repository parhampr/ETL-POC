# Zara ETL POC - Hybrid Airflow + DocETL

A complete proof-of-concept implementation for automated scientific article generation from arXiv papers using Apache Airflow orchestration with DocETL's advanced document processing capabilities.

## 🚀 Quick Start

```bash
# Clone and start the POC
git clone <your-repo>
cd zara-etl-poc

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Start all services
docker-compose up -d

# Check services are running
docker-compose ps

# Access Airflow UI
open http://localhost:8080
# Default credentials: admin/admin

# Trigger the pipeline
# Go to Airflow UI -> DAGs -> zara_hybrid_etl -> Trigger DAG
```

## 📋 Prerequisites

- Docker & Docker Compose
- OpenAI API key (for DocETL)
- At least 8GB RAM recommended
- 10GB free disk space

## 🏗️ Architecture Overview

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   arXiv API     │    │ DocETL Processing│    │   Generated     │
│   Ingestion     │───▶│  - PDF Extract   │───▶│    Articles     │
│  (Airflow)      │    │  - LLM Generation│    │  + Metadata     │
└─────────────────┘    │  - Validation    │    └─────────────────┘
         │              └──────────────────┘             │
         ▼                       │                       ▼
┌─────────────────┐              ▼              ┌─────────────────┐
│   PostgreSQL    │    ┌──────────────────┐    │   File System   │
│   (Metadata)    │    │   Monitoring     │    │ (Papers/Articles)│
└─────────────────┘    │   & Logging      │    └─────────────────┘
                       └──────────────────┘
```

## 📁 Project Structure

```
zara-etl-poc/
├── README.md
├── QUICKSTART.md
├── docker-compose.yml
├── .env.example
├── .env
├── requirements.txt
├── Dockerfile.airflow
├── Dockerfile.docetl
├── airflow/
│   ├── dags/
│   │   └── zara_hybrid_etl.py
│   ├── plugins/
│   │   ├── __init__.py
│   │   ├── arxiv_hook.py
│   │   └── docetl_operator.py
│   ├── config/
│   │   └── airflow.cfg
│   └── logs/              # Generated dynamically
├── docetl/
│   ├── configs/
│   │   ├── paper_extraction.yaml
│   │   └── article_generation.yaml
│   └── scripts/
│       └── run_pipeline.py
├── data/                   # Generated dynamically
│   ├── input/              # Downloaded arXiv papers
│   ├── processed/          # DocETL intermediate results
│   ├── output/             # Generated articles
│   └── errors/             # Error logs and debugging info
├── logs/                   # Generated dynamically
│   ├── airflow/
│   └── docetl/
└── scripts/
    ├── setup.sh
    ├── test_pipeline.sh
    ├── monitor.sh
    └── download_samples.py  # Generated by setup.sh
```

## 🛠️ Implementation Details

### Core Components

1. **Airflow Orchestration**: Handles scheduling, monitoring, and workflow management
2. **DocETL Processing**: Advanced document extraction and article generation
3. **PostgreSQL**: Metadata and pipeline state storage
4. **File System**: Paper and article storage with organized structure

### Key Features

- ✅ Automated arXiv paper ingestion
- ✅ Intelligent PDF text extraction
- ✅ LLM-powered article generation with validation
- ✅ Cost optimization through smart batching
- ✅ Comprehensive logging and monitoring
- ✅ Quality scoring and filtering
- ✅ Containerized deployment

## 🔧 Configuration

### Environment Variables (.env)
```bash
# LLM Configuration
OPENAI_API_KEY=your_openai_api_key_here
DEFAULT_MODEL=gpt-4o-mini
MAX_TOKENS=4000

# arXiv Configuration  
ARXIV_MAX_RESULTS=10
ARXIV_CATEGORIES=cs.AI,cs.CL,cs.LG

# Processing Configuration
BATCH_SIZE=5
QUALITY_THRESHOLD=0.7

# Storage Configuration
DATA_VOLUME=./data
LOGS_VOLUME=./logs

# Database Configuration
POSTGRES_DB=airflow
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
```

### DocETL Pipeline Configuration
The POC includes pre-configured DocETL pipelines for:
- Scientific paper content extraction
- Article generation with validation
- Quality scoring and filtering

## 📊 Monitoring & Debugging

### Airflow UI (http://localhost:8080)
- Monitor DAG runs and task status
- View detailed logs for each task
- Trigger manual runs and view results
- Check resource usage and performance

### Log Files
```bash
# View all logs
docker-compose logs -f

# Airflow logs only
docker-compose logs -f airflow-webserver

# DocETL processing logs
docker-compose logs -f docetl-worker

# Database logs
docker-compose logs -f postgres
```

### Debug Commands
```bash
# Enter Airflow container for debugging
docker-compose exec airflow-webserver bash

# Check DocETL pipeline directly
docker-compose exec docetl-worker python -m docetl.cli run /configs/paper_extraction.yaml

# Check database
docker-compose exec postgres psql -U airflow -d airflow

# View processed data
ls -la data/output/
```

## 🎯 Usage Examples

### Basic Pipeline Run
```bash
# Start services
docker-compose up -d

# Wait for Airflow to initialize (2-3 minutes)
# Check: http://localhost:8080

# Trigger pipeline via UI or CLI
docker-compose exec airflow-webserver airflow dags trigger zara_hybrid_etl
```

### Processing Custom Papers
```bash
# Add your PDF files to data/input/
cp your_papers/*.pdf data/input/

# Run processing
docker-compose exec docetl-worker python scripts/run_pipeline.py --input /data/input --output /data/output
```

### Quality Analysis
```bash
# View generated articles with quality scores
cat data/output/articles_with_scores.json | jq '.[] | {title, quality_score}'

# Filter high-quality articles only
cat data/output/articles_with_scores.json | jq '.[] | select(.quality_score > 0.8)'
```

## 📈 Performance & Costs

### Expected Performance
- **Processing Speed**: ~3-5 minutes per paper (depending on length)
- **Batch Processing**: 10 papers in ~30-45 minutes
- **Quality Rate**: ~70-80% articles pass quality threshold
- **Cost**: ~$0.50-2.00 per article (depending on model and length)

### Optimization Features
- Intelligent chunking to minimize token usage
- Validation loops to prevent regeneration costs
- Batch processing to optimize API calls
- Caching for repeated patterns

## 🐛 Troubleshooting

### Common Issues

**Services won't start:**
```bash
# Check Docker resources
docker system df
docker system prune  # If needed

# Check port conflicts
lsof -i :8080  # Airflow UI
lsof -i :5432  # PostgreSQL
```

**DocETL processing fails:**
```bash
# Check API key
docker-compose exec docetl-worker env | grep OPENAI_API_KEY

# Test DocETL directly
docker-compose exec docetl-worker python -c "import docetl; print('DocETL OK')"

# Check model access
docker-compose exec docetl-worker python -c "
import openai
client = openai.OpenAI()
print(client.models.list())
"
```

**Pipeline execution errors:**
```bash
# Check Airflow task logs
# Go to Airflow UI -> DAGs -> zara_hybrid_etl -> Graph View -> Click failed task -> Logs

# Check database connectivity
docker-compose exec airflow-webserver airflow db check

# Reset database if needed
docker-compose exec airflow-webserver airflow db reset
```

**Low quality scores:**
```bash
# Review generated articles
cat data/output/failed_articles.json | jq '.[0]'

# Adjust quality thresholds in .env
# Restart services
docker-compose restart
```

## 🧪 Testing

### Unit Tests
```bash
# Run all tests
docker-compose exec airflow-webserver python -m pytest /opt/airflow/tests/

# Test specific components
docker-compose exec docetl-worker python -m pytest tests/test_extraction.py
```

### Integration Tests
```bash
# Test full pipeline with sample data
./scripts/test_pipeline.sh

# Expected output:
# ✅ Services started successfully
# ✅ Sample papers processed
# ✅ Articles generated
# ✅ Quality scores above threshold
```

### Load Testing
```bash
# Process larger batch
ARXIV_MAX_RESULTS=50 docker-compose up -d
```

## 🔄 Development Workflow

### Making Changes
```bash
# Edit DAG files
vim airflow/dags/zara_hybrid_etl.py

# Edit DocETL configs  
vim docetl/configs/paper_extraction.yaml

# Restart services to apply changes
docker-compose restart airflow-webserver airflow-scheduler
```

### Adding New Features
1. Develop in local containers
2. Test with sample data
3. Update configurations
4. Document changes

## 📚 Next Steps

### POC Extensions
- [ ] Add image generation for articles
- [ ] Implement CMS publishing integration
- [ ] Add more LLM providers (Anthropic, local models)
- [ ] Enhanced quality metrics
- [ ] Real-time monitoring dashboard

### Production Readiness
- [ ] Security hardening
- [ ] Resource optimization
- [ ] High availability setup
- [ ] Backup and recovery
- [ ] CI/CD pipeline

## 🤝 Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/new-feature`)
3. Make changes and test thoroughly
4. Submit pull request

## 📝 License

MIT License - see LICENSE file for details

## 🆘 Support

For issues and questions:
1. Check logs: `docker-compose logs -f`
2. Review troubleshooting section
3. Check Airflow UI for task details
4. Create GitHub issue with logs and configuration