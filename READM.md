# Zara ETL POC - Hybrid Airflow + DocETL

A complete proof-of-concept implementation for automated scientific article generation from arXiv papers using Apache Airflow orchestration with DocETL's advanced document processing capabilities.

## ğŸš€ Quick Start

```bash
# Clone and start the POC
git clone <your-repo>
cd zara-etl-poc

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Start all services
docker-compose up -d

# Check services are running
docker-compose ps

# Access Airflow UI
open http://localhost:8080
# Default credentials: admin/admin

# Trigger the pipeline
# Go to Airflow UI -> DAGs -> zara_hybrid_etl -> Trigger DAG
```

## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- OpenAI API key (for DocETL)
- At least 8GB RAM recommended
- 10GB free disk space

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   arXiv API     â”‚    â”‚ DocETL Processingâ”‚    â”‚   Generated     â”‚
â”‚   Ingestion     â”‚â”€â”€â”€â–¶â”‚  - PDF Extract   â”‚â”€â”€â”€â–¶â”‚    Articles     â”‚
â”‚  (Airflow)      â”‚    â”‚  - LLM Generationâ”‚    â”‚  + Metadata     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - Validation    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
         â–¼                       â”‚                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â–¼              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   File System   â”‚
â”‚   (Metadata)    â”‚    â”‚   Monitoring     â”‚    â”‚ (Papers/Articles)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   & Logging      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
zara-etl-poc/
â”œâ”€â”€ README.md
â”œâ”€â”€ QUICKSTART.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile.airflow
â”œâ”€â”€ Dockerfile.docetl
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ zara_hybrid_etl.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ arxiv_hook.py
â”‚   â”‚   â””â”€â”€ docetl_operator.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ airflow.cfg
â”‚   â””â”€â”€ logs/              # Generated dynamically
â”œâ”€â”€ docetl/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ paper_extraction.yaml
â”‚   â”‚   â””â”€â”€ article_generation.yaml
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ run_pipeline.py
â”œâ”€â”€ data/                   # Generated dynamically
â”‚   â”œâ”€â”€ input/              # Downloaded arXiv papers
â”‚   â”œâ”€â”€ processed/          # DocETL intermediate results
â”‚   â”œâ”€â”€ output/             # Generated articles
â”‚   â””â”€â”€ errors/             # Error logs and debugging info
â”œâ”€â”€ logs/                   # Generated dynamically
â”‚   â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ docetl/
â””â”€â”€ scripts/
    â”œâ”€â”€ setup.sh
    â”œâ”€â”€ test_pipeline.sh
    â”œâ”€â”€ monitor.sh
    â””â”€â”€ download_samples.py  # Generated by setup.sh
```

## ğŸ› ï¸ Implementation Details

### Core Components

1. **Airflow Orchestration**: Handles scheduling, monitoring, and workflow management
2. **DocETL Processing**: Advanced document extraction and article generation
3. **PostgreSQL**: Metadata and pipeline state storage
4. **File System**: Paper and article storage with organized structure

### Key Features

- âœ… Automated arXiv paper ingestion
- âœ… Intelligent PDF text extraction
- âœ… LLM-powered article generation with validation
- âœ… Cost optimization through smart batching
- âœ… Comprehensive logging and monitoring
- âœ… Quality scoring and filtering
- âœ… Containerized deployment

## ğŸ”§ Configuration

### Environment Variables (.env)
```bash
# LLM Configuration
OPENAI_API_KEY=your_openai_api_key_here
DEFAULT_MODEL=gpt-4o-mini
MAX_TOKENS=4000

# arXiv Configuration  
ARXIV_MAX_RESULTS=10
ARXIV_CATEGORIES=cs.AI,cs.CL,cs.LG

# Processing Configuration
BATCH_SIZE=5
QUALITY_THRESHOLD=0.7

# Storage Configuration
DATA_VOLUME=./data
LOGS_VOLUME=./logs

# Database Configuration
POSTGRES_DB=airflow
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
```

### DocETL Pipeline Configuration
The POC includes pre-configured DocETL pipelines for:
- Scientific paper content extraction
- Article generation with validation
- Quality scoring and filtering

## ğŸ“Š Monitoring & Debugging

### Airflow UI (http://localhost:8080)
- Monitor DAG runs and task status
- View detailed logs for each task
- Trigger manual runs and view results
- Check resource usage and performance

### Log Files
```bash
# View all logs
docker-compose logs -f

# Airflow logs only
docker-compose logs -f airflow-webserver

# DocETL processing logs
docker-compose logs -f docetl-worker

# Database logs
docker-compose logs -f postgres
```

### Debug Commands
```bash
# Enter Airflow container for debugging
docker-compose exec airflow-webserver bash

# Check DocETL pipeline directly
docker-compose exec docetl-worker python -m docetl.cli run /configs/paper_extraction.yaml

# Check database
docker-compose exec postgres psql -U airflow -d airflow

# View processed data
ls -la data/output/
```

## ğŸ¯ Usage Examples

### Basic Pipeline Run
```bash
# Start services
docker-compose up -d

# Wait for Airflow to initialize (2-3 minutes)
# Check: http://localhost:8080

# Trigger pipeline via UI or CLI
docker-compose exec airflow-webserver airflow dags trigger zara_hybrid_etl
```

### Processing Custom Papers
```bash
# Add your PDF files to data/input/
cp your_papers/*.pdf data/input/

# Run processing
docker-compose exec docetl-worker python scripts/run_pipeline.py --input /data/input --output /data/output
```

### Quality Analysis
```bash
# View generated articles with quality scores
cat data/output/articles_with_scores.json | jq '.[] | {title, quality_score}'

# Filter high-quality articles only
cat data/output/articles_with_scores.json | jq '.[] | select(.quality_score > 0.8)'
```

## ğŸ“ˆ Performance & Costs

### Expected Performance
- **Processing Speed**: ~3-5 minutes per paper (depending on length)
- **Batch Processing**: 10 papers in ~30-45 minutes
- **Quality Rate**: ~70-80% articles pass quality threshold
- **Cost**: ~$0.50-2.00 per article (depending on model and length)

### Optimization Features
- Intelligent chunking to minimize token usage
- Validation loops to prevent regeneration costs
- Batch processing to optimize API calls
- Caching for repeated patterns

## ğŸ› Troubleshooting

### Common Issues

**Services won't start:**
```bash
# Check Docker resources
docker system df
docker system prune  # If needed

# Check port conflicts
lsof -i :8080  # Airflow UI
lsof -i :5432  # PostgreSQL
```

**DocETL processing fails:**
```bash
# Check API key
docker-compose exec docetl-worker env | grep OPENAI_API_KEY

# Test DocETL directly
docker-compose exec docetl-worker python -c "import docetl; print('DocETL OK')"

# Check model access
docker-compose exec docetl-worker python -c "
import openai
client = openai.OpenAI()
print(client.models.list())
"
```

**Pipeline execution errors:**
```bash
# Check Airflow task logs
# Go to Airflow UI -> DAGs -> zara_hybrid_etl -> Graph View -> Click failed task -> Logs

# Check database connectivity
docker-compose exec airflow-webserver airflow db check

# Reset database if needed
docker-compose exec airflow-webserver airflow db reset
```

**Low quality scores:**
```bash
# Review generated articles
cat data/output/failed_articles.json | jq '.[0]'

# Adjust quality thresholds in .env
# Restart services
docker-compose restart
```

## ğŸ§ª Testing

### Unit Tests
```bash
# Run all tests
docker-compose exec airflow-webserver python -m pytest /opt/airflow/tests/

# Test specific components
docker-compose exec docetl-worker python -m pytest tests/test_extraction.py
```

### Integration Tests
```bash
# Test full pipeline with sample data
./scripts/test_pipeline.sh

# Expected output:
# âœ… Services started successfully
# âœ… Sample papers processed
# âœ… Articles generated
# âœ… Quality scores above threshold
```

### Load Testing
```bash
# Process larger batch
ARXIV_MAX_RESULTS=50 docker-compose up -d
```

## ğŸ”„ Development Workflow

### Making Changes
```bash
# Edit DAG files
vim airflow/dags/zara_hybrid_etl.py

# Edit DocETL configs  
vim docetl/configs/paper_extraction.yaml

# Restart services to apply changes
docker-compose restart airflow-webserver airflow-scheduler
```

### Adding New Features
1. Develop in local containers
2. Test with sample data
3. Update configurations
4. Document changes

## ğŸ“š Next Steps

### POC Extensions
- [ ] Add image generation for articles
- [ ] Implement CMS publishing integration
- [ ] Add more LLM providers (Anthropic, local models)
- [ ] Enhanced quality metrics
- [ ] Real-time monitoring dashboard

### Production Readiness
- [ ] Security hardening
- [ ] Resource optimization
- [ ] High availability setup
- [ ] Backup and recovery
- [ ] CI/CD pipeline

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/new-feature`)
3. Make changes and test thoroughly
4. Submit pull request

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ†˜ Support

For issues and questions:
1. Check logs: `docker-compose logs -f`
2. Review troubleshooting section
3. Check Airflow UI for task details
4. Create GitHub issue with logs and configuration