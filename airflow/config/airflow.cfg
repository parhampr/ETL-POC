[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# Store serialized DAGs in the database
store_serialized_dags = True

# Minimum interval for serialized DAG updates
min_serialized_dag_update_interval = 30

# The folder where your airflow DAGs are stored
dags_folder = /opt/airflow/dags

# The folder where airflow plugins are stored
plugins_folder = /opt/airflow/plugins

# Base log folder
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# The executor class that airflow should use
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# Whether to load the default pool
load_default_connections = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 1

# Whether DAGs are paused by default at creation
dags_are_paused_at_creation = True

# When a task is killed, the task runner checks this interval
killed_task_cleanup_time = 60

# The amount of parallelism as a setting to the executor
parallelism = 16

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# The class to use for running task instances in a subprocess.
task_runner = StandardTaskRunner

# If set to True, the task runner will run the task in a subprocess.
run_as_user = 50000

# Default timezone in case supplied date times are naive
default_timezone = utc

# The encoding for the python source files
default_encoding = utf-8

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# Default DAG view. Valid values are: tree, graph, duration, gantt, landing_times
default_dag_view = graph

# Default DAG orientation. Valid values are: LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Expose the configuration file in the web server
expose_config = True

# Set to true to turn on authentication
authenticate = True

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Enable werkzeug debug mode
debug = False

# Expose stacktrace in the web server
expose_stacktrace = True

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers
worker_refresh_interval = 30

# If set to True, Airflow will track files in plugins_folder directory
reload_on_plugin_change = False

# Secret key used to run your flask app
secret_key = zara_etl_poc_secret_key_2025

[scheduler]
# Task instances listen for external kill signal (when you `airflow tasks test`),
# this defines the frequency at which they should listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# after how much time should the scheduler terminate in seconds
# -1 indicates to run continuously
run_duration = -1

# after how much time a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often should stats be printed to the logs
print_stats_interval = 30

# The scheduler can run multiple threads in parallel to schedule dags
max_threads = 2

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How often should the scheduler check for orphaned tasks and SchedulerJobs
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False
catchup_by_default = False

# This changes the batch size of queries in the scheduling main loop
max_tis_per_query = 512

# Should the scheduler attempt to calculate the number of tasks it can process?
use_row_level_locking = True

# How many seconds to wait between file-parsing loops to prevent the logs from
# being spammed.
parsing_processes = 2

# Turn off scheduler use of cron intervals by setting this to False
use_job_schedule = True

# Allow the use of cron presets names
allow_trigger_in_future = False

[celery]
# This section only applies if you are using the CeleryExecutor in
# [core] section above

# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# "airflow celery worker" command. This defines the number of task instances that
# a celery worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# The maximum and minimum concurrency that will be used when starting workers with the
# "airflow celery worker" command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# worker_autoscale = 16,12

# Used to set the hostname of Celery worker
worker_hostname = %h

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL
broker_url = redis://redis:6379/0

# The Celery result backend
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery
flower_host = 0.0.0.0

# The root URL for Flower
flower_url_prefix =

# This defines the port that Celery Flower runs on
flower_port = 5555

# Securing Flower
flower_basic_auth =

# Default queue that tasks get assigned to and that worker listen on
default_queue = default

# How many processes CeleryExecutor uses to sync task state
sync_parallelism = 0

# Import path for the celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# In case of using SSL
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

[operators]
# The default owner assigned to each new operator, unless provided explicitly or passed via `default_args`
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[hive]
# Default mapreduce queue for HiveOperator tasks
default_hive_mapred_queue =

[webserver]
# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =
web_server_ssl_key =

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers
worker_refresh_interval = 6000

# Secret key to save Airflow passwords in Variable
fernet_key =

# Whether to disable pickling dags
pickle_info_timeout = 30

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set to True, the task runner will run the task in a subprocess
# If set to False, the task runner will run the task in the same process
task_adoption_timeout = 600

# Time in seconds after which adopted tasks are cleared by CeleryExecutor
task_publish_max_retries = 3

# Worker initialisation check to validate Airflow version and plugins
worker_precheck = False

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_user =
smtp_password =
smtp_port = 587
smtp_mail_from = airflow@example.com

[sentry]
# Sentry (https://sentry.io) integration
sentry_dsn =

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

# Sensitive list of variables to hide from UI
sensitive_var_conn_names =

[elasticsearch]
# Elasticsearch host
host =

# Format of the log_id, which is used to query for a given tasks logs
log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}

# Used to mark the end of a log stream for a task
end_of_log_mark = end_of_log

# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
# Code will construct log_id using the log_id template from the argument above.
# NOTE: The code will prefix the https:// automatically, don't include that here.
frontend =

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository =
worker_container_tag =
worker_container_image_pull_policy = IfNotPresent

# If True (default), worker pods will be deleted upon termination
delete_worker_pods = True

# If False (default), a random worker pod will be created for each task
# If True, the scheduler will attempt to assign tasks to specific worker pods
multi_namespace_mode = False

# The Kubernetes namespace where airflow workers should be created. Defaults to `default`
namespace = default

# The Key-value pairs to be given to worker pods
worker_pods_creation_batch_size = 1

# Worker pod mutation hook
worker_airflow_home = /tmp/airflow

# Path to the kubernetes config file, if specified; otherwise, we assume you're running
# in a pod and volume mount the appropriate config file. See the execution_config_file
# for specifics on how to set this config
config_file =

# The value to use as image pull secrets
image_pull_secrets =

# GCP Service Account Keys to be provided to tasks run on Kubernetes Executor
# Should be supplied in the format: key-name-1:key-path-1,key-name-2:key-path-2
gcp_service_account_keys =

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like `kubectl` has.
# Specify the cluster_context to use when connecting to Kubernetes API.
cluster_context =

# Specify the verify_ssl parameter for the Kubernetes connection.
verify_ssl =

[kubernetes_secrets]
# The Key Vault from which to get the Connection
sql_alchemy_conn_secret =
# The Key Vault from which to get the Variable
variables_secret =
# The Key Vault from which to get the config
config_secret =

[logging]
# The folder where airflow plugins are stored
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
remote_logging = False

# Users can set a custom log class
logging_config_class =

# Log level
logging_level = INFO

# Log format for file logs
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log

# Processor log filename format
log_processor_filename_template = {{ filename }}.log

# Name of handler to read task instance logs.
task_log_reader = task

[metrics]
# StatsD (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[lineage]
# what lineage backend to use
backend =

[atlas]
sasl_enabled = False
host =
port = 21000
username =
password =

[api]
# How to authenticate users of the API. See
# https://airflow.apache.org/docs/apache-airflow/stable/security/api.html for possible values.
# ("airflow.api.auth.backend.default" allows all requests for historic reasons)
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

# Used to set the maximum page limit for API requests
maximum_page_limit = 100

# Used to set the default page limit when limit is zero. A default limit
# is required for pagination as some backends do not handle limit=0 correctly
fallback_page_limit = 100

# The intended audience for JWT token credentials used for authorization. This value must match on the client and server sides. If empty, audience will not be tested.
access_control_allow_origins = *
access_control_allow_methods = GET, POST, PUT, DELETE, OPTIONS
access_control_allow_headers = Origin, Content-Type, Accept, Authorization, X-Requested-With, X-CSRFToken

[lineage]
# what lineage backend to use
backend =

[email]
email_backend = airflow.utils.email.send_email_smtp
email_conn_id = smtp_default
default_email_on_retry = True
default_email_on_failure = True

[secrets]
# Full class name of secrets backend to enable (will precede env vars and metastore in search path)
backend =

# The backend_kwargs param is loaded into a dictionary and passed to the __init__ of secrets backend class.
# See documentation for the secrets backend you are using. JSON is expected.
backend_kwargs =